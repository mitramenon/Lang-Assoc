---
title: "Extracting data from raster and from polygons"
Date: 19-jan
output: html_notebook
---



```{r}
library(raster)
library(sp)
library(rgdal)
library(rgeos)
library(prevR)
library(amap)
library(geosphere)

#install.packages("remotes")
#remotes::install_github("kapitzas/WorldClimTiles")
library(WorldClimTiles) #if not install uncomment the above two lines and run them first
```

Load in your dataset containing lat-long information
```{r}
Landrace<-read.csv("~/Google Drive File Stream/My Drive/Language_assoc/LandraceInfo.csv")

```

# Getting worldClim bioclim data downloaded #

*See here for what the variables mean: https://worldclim.org/data/bioclim.html*
Data needs to be downloaded for 30 arc sec to your personal laptop. This is the finest resolution available and will take some time to download for the whole world. Since we only need a small area we can use the `tile_get` to determine what regions we need

```{r}
boundary <- getData("GADM", country = "MX", level = 0)
tilenames <- tile_name(boundary, name = 'worldclim')
tilenames

setwd("~/Google Drive File Stream/My Drive/Language_assoc/")
wctiles <- tile_get(tiles = tilenames, name = 'worldclim', var = "bio")
```

The command above will download to your current directory by creating a folder called wc0.5.
Combine the three tiles for each bioclim variable
```{r}
setwd("~/Google Drive File Stream/My Drive/Language_assoc/wc0.5/")


AllTiles<-vector("list",19)
bioclims<-paste0(rep("bio",19),seq(1,19),"_")

for (f in 1:length(AllTiles)){
  
  cat("working on bioClim", bioclims[f],"\n")
  bio<-list.files("./",bioclims[f])
  bio<-bio[grepl(".bil",bio)]
  bio<-lapply(bio,function(X) raster(X))
  AllTiles[[f]]<-do.call(merge,bio)
}

names(AllTiles)<-bioclims

```

Check if stuff looks as expected
```{r}
plot(AllTiles[[1]])
```

Create a raster stack and extract values for all lat-longs each raster
```{r}
S<-stack(AllTiles)
pts<-SpatialPoints(Landrace[ ,c("Longitude","Latitude")])
clim<-extract(S,pts)
clim<-cbind(Landrace,clim)
```


# Getting language data.#


Here the data is of the from SpatialPolygons, so we use a different set of commands to read in the dataset
```{r}
setwd("~/Google Drive File Stream/My Drive/Language_assoc")
lang<-readOGR(dsn=path.expand("indigenousLanguages"))
polys = attr(lang,'polygons')
names(polys)<-lang$Name


```

Determine if point is within a polygon
```{r}
npolys = length(polys)
polyID<-vector("list",npolys)
for (i in 1:npolys){
  poly = polys[[i]]
  polys2 = attr(poly,'Polygons')
  npolys2 = length(polys2)
  for (j in 1:npolys2){
     
     coords = coordinates(polys2[[j]])
     out<-point.in.polygon(Landrace$Longitude,Landrace$Latitude,coords[ ,1],coords[ ,2])
     
  }
  polyID[[i]]<-out
  
  
}
```

Extract the language of the sample location & keep only languages are present in atleast one location
```{r}
names(polyID)<-lang$Name
polyID<-do.call(cbind,polyID)
polyID_pr<-polyID[ ,colSums(polyID)!=0]
rownames(polyID_pr)<-Landrace$GBS.Taxon.ID
head(polyID_pr)
write.table(polyID_pr,file="~/Google Drive File Stream/My Drive/Language_assoc/LanguagesByLandrace.txt",sep="\t",row.names = T,quote=F)

```

# Caluclate distance measures for all our covariates # 

1) Distance for climate data. This is numeric continous. We only need the 19 bioclims and elevation and we will use euclidean distance.
```{r}

```

2) Distance metric to use for ordinal data such as the 0,1,2,3 from language. 
Several approaches :
-Recode 0 as 4 and use kendall distance.
-Recode all values other than 1 to 0 and use euclidean
-Recode 2 & 3 as 0.5 and then use euclidean

*Let's go with the 3rd approach for now*
```{r}

```

3) Geodesic Distance metric for lat-long on WGS84
```{r}
df = clim[ ,c("Longitude","Latitude")] # the order should be longitude, latitude
GeoDist<-distm(df, df, distGeo)
rownames(GeoDist)<-colnames(GeoDist)
dim(GeoDist)


```
